{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4990d6bb",
   "metadata": {},
   "source": [
    "# Student Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10880d8f",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Name:** Hoai Nhan Nguyen <br>\n",
    "**Student Number:** sba24098 <br>\n",
    "**Course:** Higher Diploma in Science in Artificial Intelligence Applications\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43ecb5",
   "metadata": {},
   "source": [
    "# Data Cleaning and Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c1c338-b3b0-483a-a0af-7e3c7a19140e",
   "metadata": {},
   "source": [
    "**Importing Apache Spark Libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6ec5ed-0709-407b-8f98-2b3b8e861513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, sum, when, regexp_replace, round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bfc4d5-0bf6-4c20-bb03-0ca96987dabd",
   "metadata": {},
   "source": [
    "**Creating a new Spark Session.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1404d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31ab9d-7164-44b6-8803-53a7e056981c",
   "metadata": {},
   "source": [
    "**Reading the Amazon-Products.csv in Hadoop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b081a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Amazon-Products.csv in Hadoop while applying options to read it correctly\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"multiLine\", \"true\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe20883-ed09-4423-9ff6-d4eb830a261e",
   "metadata": {},
   "source": [
    "**Understanding the structure of the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d3ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewing the schema of the Spark Dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190750ef-e7ca-4ac0-aab1-8ac1e4baaa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns that are not required for this task \n",
    "df = df.drop(\"_c0\",\"image\",\"link\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4831d-aa1f-4c33-9f9d-3e439f9839ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewing the rows of the Spark dataframe \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e910f74-6b7c-4256-8ff6-19b82200b754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (495197, 7)\n"
     ]
    }
   ],
   "source": [
    "# Get the number of rows\n",
    "num_rows = df.count()\n",
    "# Get the number of columns\n",
    "num_columns = len(df.columns)\n",
    "\n",
    "# Printing the shape (rows, columns)\n",
    "print(f\"Shape of the DataFrame: ({num_rows}, {num_columns})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a620b7-2616-4d8b-9a9c-cd4a43c6be31",
   "metadata": {},
   "source": [
    "**Checking the null values in the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8817d-f227-4bca-968f-cc8d452a4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the null or empty values per column\n",
    "df.select([\n",
    "    sum(when(col(column_name).isNull() | (col(column_name) == \"\"), 1).otherwise(0)).alias(column_name + \"_nulls\")\n",
    "    for column_name in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a6afc-0c41-4cc2-92cb-9ffae6040a0c",
   "metadata": {},
   "source": [
    "**Handling Null values in the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a3d40-cf86-4065-9344-16ddcbc5d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out rows where both 'discount_price' and 'actual_price' are null.\n",
    "df_clean = df.filter(~(col(\"discount_price\").isNull() & col(\"actual_price\").isNull()))\n",
    "\n",
    "# Filling the null values for ratings, no_of_ratings, discount_price and actual_price to 0\n",
    "df_clean = df_clean.fillna({\"ratings\": 0,\"no_of_ratings\": 0, \"discount_price\":0, \"actual_price\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7842589-d38f-4377-9a38-665f8e74d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the null or empty values per column\n",
    "df_clean.select([\n",
    "    sum(when(col(column_name).isNull() | (col(column_name) == \"\"), 1).otherwise(0)).alias(column_name + \"_nulls\")\n",
    "    for column_name in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5c46e-c03d-427f-aca6-43a0dffd90e3",
   "metadata": {},
   "source": [
    "**Handling duplicate rows in the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc40f86-5430-466d-8c6e-1a93e8197bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the duplicate rows from the Spark DataFrame \n",
    "df_clean = df_clean.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c2703-939f-41d3-b083-20505a8240cd",
   "metadata": {},
   "source": [
    "**Cleaning the ratings and no_of_rating columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e79a28-64d3-4802-af5b-4a2a031999a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring the rows where the 'ratings' column contains valid numbers (integers or decimals).\n",
    "df_clean = df_clean.filter(F.col('ratings').rlike(r'^[0-9]*\\.?[0-9]+$'))\n",
    "\n",
    "# Ensuring ratings are between 0 and 5.0\n",
    "df_clean = df_clean.filter((F.col('ratings') >= 0) & (F.col('ratings') <= 5.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ddbc4-00de-4a0d-930e-d5ccf55bcdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing commas from 'no_of_ratings'\n",
    "df_clean = df_clean.withColumn(\"no_of_ratings\", regexp_replace(col(\"no_of_ratings\"), \",\", \"\"))\n",
    "\n",
    "# Ensuring the rows where the 'no_of_ratings' column contains valid numbers (integers).\n",
    "df_clean = df_clean.filter(col(\"no_of_ratings\").rlike(\"^[0-9]+$\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6891ecd-f221-4a00-b1a4-38ab18425105",
   "metadata": {},
   "source": [
    "**Converting currency from Indian Rupee to Euro for the actual_price and discount_price columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e0b0fe-9222-4273-b362-b5b2d3ad90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing ₹, commas and convert to double\n",
    "df_converted = df_clean.withColumn(\n",
    "    \"actual_price\",\n",
    "    regexp_replace(col(\"actual_price\"), \"[₹,]\", \"\").cast(\"double\")  \n",
    ")\n",
    "\n",
    "# Converting INR to EUR (using conversion rate: 1 INR = 0.011 EUR)\n",
    "conversion_rate = 0.011\n",
    "df_converted = df_converted.withColumn(\n",
    "    \"actual_price\",\n",
    "    round(col(\"actual_price\") * conversion_rate, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1ee81-40cb-4b07-934f-57e5754a2ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing ₹, commas and convert to double\n",
    "df_converted = df_converted.withColumn(\n",
    "    \"discount_price\",\n",
    "    regexp_replace(col(\"discount_price\"), \"[₹,]\", \"\").cast(\"double\") \n",
    ")\n",
    "\n",
    "# Converting INR to EUR (using conversion rate: 1 INR = 0.011 EUR)\n",
    "conversion_rate = 0.011\n",
    "df_converted = df_converted.withColumn(\n",
    "    \"discount_price\",\n",
    "    round(col(\"discount_price\") * conversion_rate, 2) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef09a99-0856-450a-9d08-ab4f6c9cf002",
   "metadata": {},
   "source": [
    "**Saving converted Spark Dataframe as a CSV file in Hadoop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250822df-52a7-4300-84c7-718c872905f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_converted.write \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"quoteAll\", \"true\") \\\n",
    "  .option(\"escape\", \"\\\"\") \\\n",
    "  .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products-Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86106ac7-3122-4fa0-b90d-5a7eae9ac16e",
   "metadata": {},
   "source": [
    "# Loading CSV Data into HBase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb656d-c4c5-4619-b160-f96963373cbc",
   "metadata": {},
   "source": [
    "**Importing HappyBase Library to connect to HBase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8f868b-c085-4df3-9083-7f488ba260c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import happybase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5743b17-f720-4ff0-aa55-1ba370375ca5",
   "metadata": {},
   "source": [
    "**Reading the Amazon-Products-Cleaned.csv in Hadoop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f825f46-cb20-4e32-a0a2-e6a63467669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Reading Amazon-Products-Cleaned.csv in Hadoop while applying options to read it correctly\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"multiLine\", \"true\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products-Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e487ab-1c87-4dd8-be7d-80b6489f02a7",
   "metadata": {},
   "source": [
    "**Reviewing the Schema of Amazon-Products-Cleaned.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b55f6f2-9b9e-4fef-be72-09bbe55aa2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- ratings: double (nullable = true)\n",
      " |-- no_of_ratings: integer (nullable = true)\n",
      " |-- discount_price: double (nullable = true)\n",
      " |-- actual_price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reviewing the schema of the Spark Dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa4c84a6-bac1-4a94-b08a-191d56d60d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------------+-------+-------------+--------------+------------+\n",
      "|                name|main_category|    sub_category|ratings|no_of_ratings|discount_price|actual_price|\n",
      "+--------------------+-------------+----------------+-------+-------------+--------------+------------+\n",
      "|Voltas 1.5 Ton 3 ...|   appliances|Air Conditioners|    0.0|            0|        329.89|      472.89|\n",
      "|Panasonic 1 Ton 4...|   appliances|Air Conditioners|    4.0|            3|        406.89|       556.6|\n",
      "|Samsung 1.5 Ton 3...|   appliances|Air Conditioners|    1.7|            2|        557.47|       579.7|\n",
      "|Usha Mist Air Icy...|   appliances|  All Appliances|    4.1|        17394|         31.89|       33.55|\n",
      "|Wonderchef Nutri-...|   appliances|  All Appliances|    3.7|        11480|         32.71|        71.5|\n",
      "|SUJATA Powermatic...|   appliances|  All Appliances|    4.4|         2848|         64.63|       89.79|\n",
      "|Bajaj MX 3 Neo St...|   appliances|  All Appliances|    3.6|          176|         11.54|        23.1|\n",
      "|Samsung 23 L Solo...|   appliances|  All Appliances|    4.2|          250|          75.9|        82.5|\n",
      "|Dyson V8 Absolute...|   appliances|  All Appliances|    4.6|         1017|         361.9|       482.9|\n",
      "|Black + Decker WD...|   appliances|  All Appliances|    3.8|          467|         32.99|       57.64|\n",
      "|AQUADOVE Universa...|   appliances|  All Appliances|    3.8|          367|           4.4|       14.85|\n",
      "|Stylista Washing ...|   appliances|  All Appliances|    4.1|         1099|          5.49|       10.99|\n",
      "|F&A | Washing Mac...|   appliances|  All Appliances|    4.2|         1674|          4.68|       10.99|\n",
      "|Brayden Fito Zapp...|   appliances|  All Appliances|    3.7|          404|         18.69|       43.99|\n",
      "|Revo Bag Closer M...|   appliances|  All Appliances|    4.2|          120|         61.71|        99.0|\n",
      "|MINI PORTABLE IRO...|   appliances|  All Appliances|    0.0|            0|          5.39|       32.99|\n",
      "|LIMBANI BROTHERS™...|   appliances|  All Appliances|    3.7|          180|         14.29|       21.99|\n",
      "|SUBHASH® 12 inch ...|   appliances|  All Appliances|    3.8|           59|         21.99|       38.49|\n",
      "|acuro CHEMIRO 909...|   appliances|  All Appliances|    3.9|           40|         17.59|       80.29|\n",
      "|Morphy Richards 3...|   appliances|  All Appliances|    3.9|          320|         20.22|       37.35|\n",
      "+--------------------+-------------+----------------+-------+-------------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9e6b1-6a06-405a-bcde-2f68bcfc3bb1",
   "metadata": {},
   "source": [
    "**Connecting to Hbase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19318bb4-090c-4bcc-9c8f-3ed4d8e1f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to HBase with the happybase library \n",
    "connection = happybase.Connection('localhost')\n",
    "connection.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab3ef53-e14d-4296-b105-a335bf948ab3",
   "metadata": {},
   "source": [
    "**Creating and adding data to the table.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "034cc815-9aca-4ae9-88ad-c18ecf55d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Defining the column families \n",
    "column_families = {\n",
    "    'Item_Info': dict(),\n",
    "    'Ratings_Info': dict(),\n",
    "    'Pricing_Info': dict(),\n",
    "}\n",
    "\n",
    "# Creating the table amazon_products if it doesn't exist\n",
    "table_name = 'amazon_products'\n",
    "if table_name not in connection.tables():\n",
    "    connection.create_table(table_name, column_families)\n",
    "\n",
    "# Getting the table amazon_products\n",
    "table = connection.table(table_name)\n",
    "\n",
    "# For loop to add the data from Amazon-Products-Cleaned.csv to the table amazon_products\n",
    "for idx, row in enumerate(df.rdd.collect(), start=1):\n",
    "    # Generating a 6-character row key for each item\n",
    "    row_key = str(idx).zfill(6)\n",
    "    \n",
    "    # Adding the data to the table\n",
    "    table.put(row_key, {\n",
    "        'Item_Info:name': row['name'],\n",
    "        'Item_Info:main_category': row['main_category'],\n",
    "        'Item_Info:sub_category': row['sub_category'],\n",
    "        'Ratings_Info:ratings': str(row['ratings']),\n",
    "        'Ratings_Info:no_of_ratings': str(row['no_of_ratings']),\n",
    "        'Pricing_Info:discount_price': str(row['discount_price']),\n",
    "        'Pricing_Info:actual_price': str(row['actual_price'])\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a484c72-6edb-476e-9e43-dc0c8dd44540",
   "metadata": {},
   "source": [
    "**Closing the connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69038c3c-cd91-45b3-9e4f-54c39f4742dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
