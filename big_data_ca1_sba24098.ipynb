{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4990d6bb",
   "metadata": {},
   "source": [
    "# Student Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10880d8f",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Name:** Hoai Nhan Nguyen <br>\n",
    "**Student Number:** sba24098 <br>\n",
    "**Course:** Higher Diploma in Science in Artificial Intelligence Applications\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43ecb5",
   "metadata": {},
   "source": [
    "# Data Cleaning and Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c1c338-b3b0-483a-a0af-7e3c7a19140e",
   "metadata": {},
   "source": [
    "**Importing Apache Spark Libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6ec5ed-0709-407b-8f98-2b3b8e861513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, sum, when, regexp_replace, round, udf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bfc4d5-0bf6-4c20-bb03-0ca96987dabd",
   "metadata": {},
   "source": [
    "**Creating a new Spark Session.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1404d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31ab9d-7164-44b6-8803-53a7e056981c",
   "metadata": {},
   "source": [
    "**Reading the Amazon-Products.csv in Hadoop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b081a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Amazon-Products.csv in Hadoop while applying options to read it correctly\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"multiLine\", \"true\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe20883-ed09-4423-9ff6-d4eb830a261e",
   "metadata": {},
   "source": [
    "**Understanding the structure of the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d3ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewing the schema of the Spark Dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190750ef-e7ca-4ac0-aab1-8ac1e4baaa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns that are not required for this task \n",
    "df = df.drop(\"_c0\",\"image\",\"link\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4831d-aa1f-4c33-9f9d-3e439f9839ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewing the rows of the Spark dataframe \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e910f74-6b7c-4256-8ff6-19b82200b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows\n",
    "num_rows = df.count()\n",
    "# Get the number of columns\n",
    "num_columns = len(df.columns)\n",
    "\n",
    "# Printing the shape (rows, columns)\n",
    "print(f\"Shape of the DataFrame: ({num_rows}, {num_columns})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a620b7-2616-4d8b-9a9c-cd4a43c6be31",
   "metadata": {},
   "source": [
    "**Checking the null values in the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8817d-f227-4bca-968f-cc8d452a4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the null or empty values per column\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a6afc-0c41-4cc2-92cb-9ffae6040a0c",
   "metadata": {},
   "source": [
    "**Handling Null values in the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a3d40-cf86-4065-9344-16ddcbc5d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out rows where both 'discount_price' and 'actual_price' are null.\n",
    "df_clean = df.filter(~(col(\"discount_price\").isNull() & col(\"actual_price\").isNull()))\n",
    "\n",
    "# Filling the null values for ratings, no_of_ratings, discount_price and actual_price to 0\n",
    "df_clean = df_clean.fillna({\"ratings\": 0,\"no_of_ratings\": 0, \"discount_price\":0, \"actual_price\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7842589-d38f-4377-9a38-665f8e74d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the null or empty values per column\n",
    "df_clean.select([\n",
    "    sum(when(col(column_name).isNull() | (col(column_name) == \"\"), 1).otherwise(0)).alias(column_name + \"_nulls\")\n",
    "    for column_name in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5c46e-c03d-427f-aca6-43a0dffd90e3",
   "metadata": {},
   "source": [
    "**Handling duplicate rows in the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc40f86-5430-466d-8c6e-1a93e8197bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the duplicate rows based on name from the Spark DataFrame \n",
    "df_clean = df_clean.dropDuplicates([\"name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c2703-939f-41d3-b083-20505a8240cd",
   "metadata": {},
   "source": [
    "**Cleaning the ratings and no_of_rating columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e79a28-64d3-4802-af5b-4a2a031999a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring the rows where the 'ratings' column contains valid numbers (integers or decimals).\n",
    "df_clean = df_clean.filter(F.col('ratings').rlike(r'^[0-9]*\\.?[0-9]+$'))\n",
    "\n",
    "# Ensuring ratings are between 0 and 5.0\n",
    "df_clean = df_clean.filter((F.col('ratings') >= 0) & (F.col('ratings') <= 5.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ddbc4-00de-4a0d-930e-d5ccf55bcdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing commas from 'no_of_ratings'\n",
    "df_clean = df_clean.withColumn(\"no_of_ratings\", regexp_replace(col(\"no_of_ratings\"), \",\", \"\"))\n",
    "\n",
    "# Ensuring the rows where the 'no_of_ratings' column contains valid numbers (integers).\n",
    "df_clean = df_clean.filter(col(\"no_of_ratings\").rlike(\"^[0-9]+$\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6891ecd-f221-4a00-b1a4-38ab18425105",
   "metadata": {},
   "source": [
    "**Converting currency from Indian Rupee to Euro for the actual_price and discount_price columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e0b0fe-9222-4273-b362-b5b2d3ad90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing ₹, commas and convert to double\n",
    "df_converted = df_clean.withColumn(\n",
    "    \"actual_price\",\n",
    "    regexp_replace(col(\"actual_price\"), \"[₹,]\", \"\").cast(\"double\")  \n",
    ")\n",
    "\n",
    "# Converting INR to EUR (using conversion rate: 1 INR = 0.011 EUR)\n",
    "conversion_rate = 0.011\n",
    "df_converted = df_converted.withColumn(\n",
    "    \"actual_price\",\n",
    "    round(col(\"actual_price\") * conversion_rate, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1ee81-40cb-4b07-934f-57e5754a2ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing ₹, commas and convert to double\n",
    "df_converted = df_converted.withColumn(\n",
    "    \"discount_price\",\n",
    "    regexp_replace(col(\"discount_price\"), \"[₹,]\", \"\").cast(\"double\") \n",
    ")\n",
    "\n",
    "# Converting INR to EUR (using conversion rate: 1 INR = 0.011 EUR)\n",
    "conversion_rate = 0.011\n",
    "df_converted = df_converted.withColumn(\n",
    "    \"discount_price\",\n",
    "    round(col(\"discount_price\") * conversion_rate, 2) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef09a99-0856-450a-9d08-ab4f6c9cf002",
   "metadata": {},
   "source": [
    "**Saving converted Spark Dataframe as a CSV file in Hadoop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250822df-52a7-4300-84c7-718c872905f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_converted.write \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"quoteAll\", \"true\") \\\n",
    "  .option(\"escape\", \"\\\"\") \\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products-Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86106ac7-3122-4fa0-b90d-5a7eae9ac16e",
   "metadata": {},
   "source": [
    "# Inserting CSV Data into HBase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb656d-c4c5-4619-b160-f96963373cbc",
   "metadata": {},
   "source": [
    "**Importing HappyBase Library to connect to HBase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f868b-c085-4df3-9083-7f488ba260c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import happybase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5743b17-f720-4ff0-aa55-1ba370375ca5",
   "metadata": {},
   "source": [
    "**Reading the Amazon-Products-Cleaned.csv in Hadoop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f825f46-cb20-4e32-a0a2-e6a63467669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Amazon-Products-Cleaned.csv in Hadoop while applying options to read it correctly\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"multiLine\", \"true\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products-Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e487ab-1c87-4dd8-be7d-80b6489f02a7",
   "metadata": {},
   "source": [
    "**Reviewing the Schema of Amazon-Products-Cleaned.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b55f6f2-9b9e-4fef-be72-09bbe55aa2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewing the schema of the Spark Dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c84a6-bac1-4a94-b08a-191d56d60d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9e6b1-6a06-405a-bcde-2f68bcfc3bb1",
   "metadata": {},
   "source": [
    "**Connecting to Hbase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19318bb4-090c-4bcc-9c8f-3ed4d8e1f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to HBase with the happybase library \n",
    "connection = happybase.Connection('localhost')\n",
    "connection.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab3ef53-e14d-4296-b105-a335bf948ab3",
   "metadata": {},
   "source": [
    "**Creating and adding data to the table.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034cc815-9aca-4ae9-88ad-c18ecf55d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the column families \n",
    "column_families = {\n",
    "    'Item_Info': dict(),\n",
    "    'Ratings_Info': dict(),\n",
    "    'Pricing_Info': dict(),\n",
    "}\n",
    "\n",
    "# Creating the table amazon_products if it doesn't exist\n",
    "table_name = 'amazon_products'\n",
    "if table_name not in connection.tables():\n",
    "    connection.create_table(table_name, column_families)\n",
    "\n",
    "# Getting the table amazon_products\n",
    "table = connection.table(table_name)\n",
    "\n",
    "# For loop to add the data from Amazon-Products-Cleaned.csv to the table amazon_products\n",
    "for idx, row in enumerate(df.rdd.collect(), start=1):\n",
    "    # Generating a 6-character row key for each item\n",
    "    row_key = str(idx).zfill(6)\n",
    "    \n",
    "    # Adding the data to the table\n",
    "    table.put(row_key, {\n",
    "        'Item_Info:name': row['name'],\n",
    "        'Item_Info:main_category': row['main_category'],\n",
    "        'Item_Info:sub_category': row['sub_category'],\n",
    "        'Ratings_Info:ratings': str(row['ratings']),\n",
    "        'Ratings_Info:no_of_ratings': str(row['no_of_ratings']),\n",
    "        'Pricing_Info:discount_price': str(row['discount_price']),\n",
    "        'Pricing_Info:actual_price': str(row['actual_price'])\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a484c72-6edb-476e-9e43-dc0c8dd44540",
   "metadata": {},
   "source": [
    "**Closing the connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69038c3c-cd91-45b3-9e4f-54c39f4742dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d726593-3995-4aaf-b8ad-818fadebebc9",
   "metadata": {},
   "source": [
    "# Apache Spark - Basic Analysis and Insights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a942728-e49f-4960-b2c8-214923408b0d",
   "metadata": {},
   "source": [
    "**Importing Libraries for visualisations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ddacd-b4b7-4bc9-bff7-fbbd8cd594e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe7442-ac49-4c3d-8458-4db7e4a81a4d",
   "metadata": {},
   "source": [
    "**Reading the Amazon-Products-Cleaned.csv in Hadoop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75717140-7c26-4211-a56d-7fa501dc5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Amazon-Products-Cleaned.csv in Hadoop while applying options to read it correctly\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"multiLine\", \"true\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products-Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4ca35-f116-4318-b0c6-672fd72be213",
   "metadata": {},
   "source": [
    "**Filtering dataframe for the top 5 for ratings >= 4.5 and number of ratings > 1000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dfa7a9-9dc2-4d59-969e-e19211ec12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataframe for the top 5 for ratings >= 4.5 and number of ratings > 1000\n",
    "df_top_5_product_review = df.filter((F.col(\"ratings\") >= 4.5) & (F.col(\"no_of_ratings\") > 1000)) \\\n",
    "  .orderBy(F.desc(\"ratings\"), F.desc(\"no_of_ratings\")) \\\n",
    "  .limit(5)\n",
    "\n",
    "df_top_5_product_review.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24641c9c-9a9a-4360-9a65-7b49e22fb59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the Apache Spark Dataframe to Panda Dataframe\n",
    "df_top_5_product_review_pd = df_top_5_product_review.toPandas()\n",
    "\n",
    "# Function to take only the first 3 words from the product name\n",
    "def first_three_words(name):\n",
    "    words = name.split()\n",
    "    return ' '.join(words[:3])\n",
    "\n",
    "# Adjusting the product name for readability \n",
    "df_top_5_product_review_pd['name'] = df_top_5_product_review_pd['name'].apply(first_three_words)\n",
    "\n",
    "# Setting the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Creating the bar chart using seaborn\n",
    "sns.barplot(\n",
    "    x=\"no_of_ratings\",\n",
    "    y=\"name\",\n",
    "    data=df_top_5_product_review_pd,\n",
    "    hue=\"name\",\n",
    "    palette=\"tab10\"\n",
    ")\n",
    "\n",
    "# Styling the plot\n",
    "plt.grid(True, which='major', linestyle='--', linewidth=0.5, zorder=0)\n",
    "plt.title(\"Top 5 Products with Highest Ratings and Over 1000 Reviews\", loc=\"center\", pad=15, fontsize=12)\n",
    "plt.xlabel(\"Number of Reviews\", loc=\"center\", labelpad=15, fontsize=8.5)\n",
    "plt.ylabel(\"\")  \n",
    "plt.tick_params(axis='both', labelsize=8.5)\n",
    "\n",
    "# Saving the plot as an image and uploading it to Hadoop\n",
    "plt.savefig(\"top_5_products_based_on_highest_rating.png\", dpi=300, bbox_inches='tight')\n",
    "subprocess.run([\"hdfs\", \"dfs\", \"-put\", \"-f\", \"top_5_products_based_on_highest_rating.png\", \"/user1/big_data_ca1/images/\"])\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4868343e-15a5-449b-b3d6-a1f9e06c1b82",
   "metadata": {},
   "source": [
    "**Filtering dataframe for the bottom 5 for ratings >= 1 and number of ratings > 1000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b9c14-f8e4-4ed1-876a-bc88ce906d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataframe for the bottom 5 for ratings >= 1 and number of ratings > 1000\n",
    "df_bottom_5_products_review = df.filter((F.col(\"ratings\") >= 1) & (F.col(\"no_of_ratings\") > 1000)) \\\n",
    "  .orderBy(F.asc(\"ratings\"), F.asc(\"no_of_ratings\")) \\\n",
    "  .limit(5)\n",
    "\n",
    "df_bottom_5_products_review.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb762f5-39da-4a94-a702-219793209ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the Apache Spark Dataframe to Panda Dataframe\n",
    "df_bottom_5_products_review_pd = df_bottom_5_products_review.toPandas()\n",
    "# Adjusting the product name for readability \n",
    "df_bottom_5_products_review_pd['name'] = df_bottom_5_products_review_pd['name'].apply(first_three_words)\n",
    "\n",
    "# Setting the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Creating the bar chart using seaborn\n",
    "sns.barplot(\n",
    "    x=\"no_of_ratings\",\n",
    "    y=\"name\",\n",
    "    data=df_bottom_5_products_review_pd,\n",
    "    hue=\"name\",\n",
    "    palette=\"tab10\"\n",
    ")\n",
    "\n",
    "# Styling the plot\n",
    "plt.grid(True, which='major', linestyle='--', linewidth=0.5, zorder=0)\n",
    "plt.title(\"Bottom 5 Product with Lowest Ratings and Over 1000 Reviews\", loc=\"center\", pad=15, fontsize=12)\n",
    "plt.xlabel(\"Number of Reviews\", loc=\"center\", labelpad=15, fontsize=8.5)\n",
    "plt.ylabel(\"\")  \n",
    "plt.tick_params(axis='both', labelsize=8.5)\n",
    "\n",
    "# Saving the plot as an image and uploading it to Hadoop\n",
    "plt.savefig(\"bottom_5_products_based_on_lowest_rating.png\", dpi=300, bbox_inches='tight')\n",
    "subprocess.run([\"hdfs\", \"dfs\", \"-put\", \"-f\", \"bottom_5_products_based_on_lowest_rating.png\", \"/user1/big_data_ca1/images/\"])\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd2ef6-05ba-4212-9f94-d3819b5eda47",
   "metadata": {},
   "source": [
    "**Grouping by 'main_category' and calculate summary statistics for prices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf589a-7936-454a-8c50-51b5c06e8a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'main_category' and calculate summary statistics for prices\n",
    "df_price_stats = df.groupBy(\"main_category\").agg(\n",
    "    F.min(\"actual_price\").alias(\"min_actual_price\"),\n",
    "    F.max(\"actual_price\").alias(\"max_actual_price\"),\n",
    "    F.percentile_approx(\"actual_price\", 0.5).alias(\"median_actual_price\"),\n",
    "    F.min(\"discount_price\").alias(\"min_discount_price\"),\n",
    "    F.max(\"discount_price\").alias(\"max_discount_price\"),\n",
    "    F.percentile_approx(\"discount_price\", 0.5).alias(\"median_discount_price\")\n",
    ").orderBy(col(\"median_actual_price\").desc())\n",
    "\n",
    "# Show dataframe\n",
    "df_price_stats.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc9183e-7e11-4b22-b7ae-235fab117fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the Apache Spark Dataframe to Panda Dataframe\n",
    "df_price_stats_pd = df_price_stats.toPandas()\n",
    "\n",
    "# Retieving the top 5 main category\n",
    "df_top_5_main_category = df_price_stats_pd.head(5)\n",
    "# Applying the main category as a title for readability \n",
    "df_top_5_main_category.loc[:, \"main_category\"] = df_top_5_main_category[\"main_category\"].apply(str.title)\n",
    "\n",
    "# Setting the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Creating the bar chart using seaborn\n",
    "sns.barplot(\n",
    "    x=\"median_actual_price\",\n",
    "    y=\"main_category\",\n",
    "    data=df_top_5_main_category,\n",
    "    hue=\"main_category\",\n",
    "    palette=\"tab10\"\n",
    ")\n",
    "\n",
    "# Styling the plot\n",
    "plt.grid(True, which='major', linestyle='--', linewidth=0.5, zorder=0)\n",
    "plt.title(\"Top 5 Main Categories Based on Median Price\", loc=\"center\", pad=15, fontsize=12)\n",
    "plt.xlabel(\"Price in Euro\", loc=\"center\", labelpad=15, fontsize=8.5)\n",
    "plt.ylabel(\"\")  \n",
    "plt.tick_params(axis='both', labelsize=8.5)\n",
    "\n",
    "# Saving the plot as an image and uploading it to Hadoop\n",
    "plt.savefig(\"top_5_main_categories_based_on_median_price.png\", dpi=300, bbox_inches='tight')\n",
    "subprocess.run([\"hdfs\", \"dfs\", \"-put\", \"-f\", \"top_5_main_categories_based_on_median_price.png\", \"/user1/big_data_ca1/images/\"])\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3e72f-ba52-4b1b-a250-ec2dc147de73",
   "metadata": {},
   "source": [
    "**Calculating the total discount loss based on actual price - discount price**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd8268f-a574-400e-957c-3154fd3a643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the total discount loss based on actual price - discount price\n",
    "total_discount_loss = df.withColumn(\"discount_loss\", F.col(\"actual_price\") - F.col(\"discount_price\")) \\\n",
    "                      .agg(\n",
    "                          F.format_number(F.sum(\"actual_price\"), 2).alias(\"total_actual_price\"),\n",
    "                          F.format_number(F.sum(\"discount_loss\"), 2).alias(\"total_discount_loss\")\n",
    "                      ) \n",
    "\n",
    "total_discount_loss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f5547-fcd6-42f6-a6f9-b80e8ba4f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the total discount loss based on actual price - discount price (without format_number)\n",
    "total_discount_loss = df.withColumn(\"discount_loss\", F.col(\"actual_price\") - F.col(\"discount_price\")) \\\n",
    "                      .agg(\n",
    "                          F.sum(\"actual_price\").alias(\"total_actual_price\"),\n",
    "                          F.sum(\"discount_loss\").alias(\"total_discount_loss\")\n",
    "                      ) \n",
    "\n",
    "# Changing the Apache Spark Dataframe to Panda Dataframe\n",
    "total_discount_loss_pd = total_discount_loss.toPandas()  \n",
    "\n",
    "# Reshape the data for calculation and amount\n",
    "total_discount_loss_reshape_sorted = pd.melt(total_discount_loss_pd, var_name=\"calculation\", value_name=\"amount\").sort_values(by=\"amount\", ascending=False)\n",
    "\n",
    "# Converting the amount value as an integer for readability \n",
    "total_discount_loss_reshape_sorted['amount'] = total_discount_loss_reshape_sorted['amount'].astype(int)\n",
    "# Applying the caculation values as a title and removing \"_\" for readability \n",
    "total_discount_loss_reshape_sorted['calculation'] = total_discount_loss_reshape_sorted['calculation'].str.replace('_', ' ').str.title()\n",
    "\n",
    "# Setting the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Creating the bar chart using seaborn\n",
    "sns.barplot(\n",
    "    x=\"amount\",\n",
    "    y=\"calculation\",\n",
    "    data=total_discount_loss_reshape_sorted,\n",
    "    hue=\"amount\",\n",
    "    palette=\"tab10\"\n",
    ")\n",
    "\n",
    "# Styling the plot\n",
    "plt.grid(True, which='major', linestyle='--', linewidth=0.5, zorder=0)\n",
    "plt.title(\"Total Price vs. Total Discount Loss Comparison\", loc=\"center\", pad=15, fontsize=12)\n",
    "plt.xlabel(\"Price in Euro\", loc=\"center\", labelpad=15, fontsize=8.5)\n",
    "plt.ylabel(\"\")  \n",
    "plt.tick_params(axis='both', labelsize=8.5)\n",
    "\n",
    "# Added a legend with title and move it outside the plot\n",
    "plt.legend(title='Amount', bbox_to_anchor=(1,1), loc='upper left')\n",
    "\n",
    "# Saving the plot as an image and uploading it to Hadoop\n",
    "plt.savefig(\"total_price_vs_total_discount_loss.png\", dpi=300, bbox_inches='tight')\n",
    "subprocess.run([\"hdfs\", \"dfs\", \"-put\", \"-f\", \"total_price_vs_total_discount_loss.png\", \"/user1/big_data_ca1/images/\"])\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5fd48b-b7ca-470d-bcdf-4528a9ddfca1",
   "metadata": {},
   "source": [
    "**Grouping by 'main_category' and calculate average rating and total number of ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f8114-3f07-4bd6-b9f7-2e5eb93cf35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'main_category' and calculate average rating and total number of ratings\n",
    "df_rating = df.groupBy(\"main_category\") \\\n",
    "  .agg(\n",
    "      F.round(F.avg(\"ratings\"), 1).alias(\"average_rating\"),\n",
    "      F.sum(\"no_of_ratings\").alias(\"total_number_ratings\")\n",
    "  ) \\\n",
    "  .orderBy(F.col(\"average_rating\").desc()) \\\n",
    "\n",
    "df_rating.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70056b29-25ce-4b67-84ca-ca53883be17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the Apache Spark Dataframe to Panda Dataframe\n",
    "df_rating_pd = df_rating.toPandas()\n",
    "\n",
    "# Retieving the top 5 main category\n",
    "df_rating_top_5_main_category = df_rating_pd.head(5)\n",
    "\n",
    "# Applying the main category as a title for readability \n",
    "df_rating_top_5_main_category.loc[:, \"main_category\"] = df_rating_top_5_main_category[\"main_category\"].apply(str.title)\n",
    "\n",
    "# Setting the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Creating the bar chart using seaborn\n",
    "sns.barplot(\n",
    "    x=\"average_rating\",\n",
    "    y=\"main_category\",\n",
    "    data=df_rating_top_5_main_category,\n",
    "    hue=\"main_category\",\n",
    "    palette=\"tab10\"\n",
    ")\n",
    "\n",
    "# Styling the plot\n",
    "plt.grid(True, which='major', linestyle='--', linewidth=0.5, zorder=0)\n",
    "plt.title(\"Top 5 Main Categories Based on Average Rating\", loc=\"center\", pad=15, fontsize=12)\n",
    "plt.xlabel(\"Average Rating\", loc=\"center\", labelpad=15, fontsize=8.5)\n",
    "plt.ylabel(\"\")  \n",
    "plt.tick_params(axis='both', labelsize=8.5)\n",
    "\n",
    "# Saving the plot as an image and uploading it to Hadoop\n",
    "plt.savefig(\"top_5_main_categories_based_on_average_rating.png\", dpi=300, bbox_inches='tight')\n",
    "subprocess.run([\"hdfs\", \"dfs\", \"-put\", \"-f\", \"top_5_main_categories_based_on_average_rating.png\", \"/user1/big_data_ca1/images/\"])\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee7fb7-abab-469d-b510-4141ea768bd7",
   "metadata": {},
   "source": [
    "# Apache Spark - Content-Based Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a67cb-3eeb-4ba0-95dd-3e7e327b9cd7",
   "metadata": {},
   "source": [
    "**Importing libraries for content-based recommendation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de298bbe-aafe-466d-9d9e-43ceee9f3914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619580b-b5d5-48f7-bbb5-4bcab7fd5de6",
   "metadata": {},
   "source": [
    "**Reading the Amazon-Products-Cleaned.csv in Hadoop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e6fe83-404c-4143-af0d-daa462d9fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Amazon-Products-Cleaned.csv in Hadoop while applying options to read it correctly\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"multiLine\", \"true\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products-Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d9e0a-f230-4ab3-a234-2f9a1c3e416b",
   "metadata": {},
   "source": [
    "**Adding product_id column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf77a4fd-345a-4dac-a31e-0cf9dc481de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = false)\n",
      " |-- name: string (nullable = true)\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- ratings: double (nullable = true)\n",
      " |-- no_of_ratings: integer (nullable = true)\n",
      " |-- discount_price: double (nullable = true)\n",
      " |-- actual_price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to the dataframe named product_id while using the monotonically_increasing_id() function\n",
    "df = df.withColumn(\"product_id\", monotonically_increasing_id())\n",
    "\n",
    "# Selecting the order of the columns of the dataframe\n",
    "df = df.select([\"product_id\", \"name\", \"main_category\", \"sub_category\", \"ratings\", \"no_of_ratings\", \"discount_price\", \"actual_price\"])\n",
    "\n",
    "# Print the the dataframe Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d3248-a4a1-4bb5-a975-8bfc08232809",
   "metadata": {},
   "source": [
    "**Transforming DataFrame into Numeric Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef0240f-ca67-48b3-8777-96fc3fb5acf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Indexing the categorical features [\"main_category\", \"sub_category\"]\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid=\"keep\") \n",
    "    for col in [\"main_category\", \"sub_category\"]\n",
    "]\n",
    "\n",
    "# Encoding the categorical features [\"main_category\", \"sub_category\"]\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=f\"{col}_idx\", outputCol=f\"{col}_vec\") \n",
    "    for col in [\"main_category\", \"sub_category\"]\n",
    "]\n",
    "\n",
    "# Assembling all numeric features into a single vector named \"features\"\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"main_category_vec\", \"sub_category_vec\", \n",
    "               \"ratings\", \"no_of_ratings\", \"discount_price\", \"actual_price\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Creating a Spark Pipeline that combines all stages (indexers, encoders, and the assembler)\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "# Fitting the pipeline to the DataFrame \n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# Applying the fitted pipeline model to the DataFrame to transform i\n",
    "df_features = model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517aedc5-b8d0-49ca-a9ec-9ab0c9169697",
   "metadata": {},
   "source": [
    "**Function to calculate similar recommended products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17567fe4-55c5-4a02-8948-13ce397d818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_products(product_id, top_n):\n",
    "    \n",
    "    # Display dataframe row for the specified product ID\n",
    "    print(f\"Product ID {product_id} information:\")\n",
    "    df.filter(df['product_id'] == product_id).show()\n",
    "    \n",
    "    # Get the row for the specified product ID based on df_features\n",
    "    product_row = df_features.filter(col(\"product_id\") == product_id).select(\"product_id\", \"features\").head()\n",
    "    # Get the feature vector for the input product\n",
    "    product_vector = product_row['features']\n",
    "\n",
    "    # Select product IDs and features (excluding the reference product itself)\n",
    "    product_features = df_features.filter(col(\"product_id\") != product_id).select(\"product_id\", \"features\")\n",
    "\n",
    "    # Function to compute cosine similarity \n",
    "    def cosine_similarity_udf(vec1, vec2):\n",
    "        dot_product = float(vec1.dot(vec2))\n",
    "        norm1 = float(vec1.norm(2))\n",
    "        norm2 = float(vec2.norm(2))\n",
    "        return dot_product / (norm1 * norm2)\n",
    "\n",
    "    # Register the UDF for cosine similarity\n",
    "    cosine_similarity = F.udf(lambda x: cosine_similarity_udf(x, product_vector), DoubleType())\n",
    "\n",
    "    # Apply the cosine similarity calculation for each product in the DataFrame\n",
    "    product_features = product_features.withColumn(\n",
    "        \"cosine_similarity_score\", cosine_similarity(col(\"features\"))\n",
    "    )\n",
    "\n",
    "    # Get the top_n most similar products by ordering by cosine similarity score\n",
    "    similar_df = product_features.orderBy(col(\"cosine_similarity_score\").desc()).limit(top_n)\n",
    "    \n",
    "    # Collect recommended product IDs\n",
    "    recommended_product_ids = [row['product_id'] for row in similar_df.select('product_id').collect()]\n",
    "    \n",
    "    # Filtering dataframe based on recommended product IDs\n",
    "    recommended_df = df.filter(df['product_id'].isin(recommended_product_ids))\n",
    "\n",
    "    # Show the filtered rows of recommended products\n",
    "    print(f\"\\nTop {top_n} recommended products based on Product ID {product_id}:\")\n",
    "    recommended_df.show()\n",
    "\n",
    "    # Show the filtered rows of the top N recommended products with their cosine similarity scores\n",
    "    print(\"Recommended Product IDs with Similarity Scores\")\n",
    "    similar_df.select(\"product_id\", \"cosine_similarity_score\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b298f-96c3-42fd-8c42-7ba45260ae12",
   "metadata": {},
   "source": [
    "**Executing recommend_similar_products function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4cbcbd1-9c67-43f4-bac9-025723915682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product ID 123456 information:\n",
      "+----------+--------------------+-------------------+------------------+-------+-------------+--------------+------------+\n",
      "|product_id|                name|      main_category|      sub_category|ratings|no_of_ratings|discount_price|actual_price|\n",
      "+----------+--------------------+-------------------+------------------+-------+-------------+--------------+------------+\n",
      "|    123456|Prolite Curtain R...|tv, audio & cameras|Camera Accessories|    4.0|           11|          8.79|        9.89|\n",
      "+----------+--------------------+-------------------+------------------+-------+-------------+--------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 recommended products based on Product ID 123456:\n",
      "+----------+--------------------+-------------------+------------------+-------+-------------+--------------+------------+\n",
      "|product_id|                name|      main_category|      sub_category|ratings|no_of_ratings|discount_price|actual_price|\n",
      "+----------+--------------------+-------------------+------------------+-------+-------------+--------------+------------+\n",
      "|    122266|Power Smart EN-EL...|tv, audio & cameras|Camera Accessories|    4.3|           13|          8.79|       10.99|\n",
      "|    172423|WELBORN Camera Ba...|tv, audio & cameras|Camera Accessories|    3.9|            9|          6.59|         7.7|\n",
      "|8590047494|Oligitdi Movie Di...|tv, audio & cameras|Camera Accessories|    4.0|           11|          8.79|        11.0|\n",
      "|8590106468|WELBORN NP-BD1 Ba...|tv, audio & cameras|Camera Accessories|    3.8|           12|          9.89|       11.66|\n",
      "|8590109147|WildRoar Camera B...|tv, audio & cameras|Camera Accessories|    4.5|           11|          9.34|        11.0|\n",
      "+----------+--------------------+-------------------+------------------+-------+-------------+--------------+------------+\n",
      "\n",
      "Recommended Product IDs with Similarity Scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|product_id|cosine_similarity_score|\n",
      "+----------+-----------------------+\n",
      "|8590109147|     0.9988977909425116|\n",
      "|8590106468|     0.9988128018831507|\n",
      "|8590047494|     0.9987414553535601|\n",
      "|    122266|     0.9979306341686875|\n",
      "|    172423|     0.9978844938350759|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Executing function for product ID 123456 and top 5 recommended products\n",
    "recommend_similar_products(123456, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119afabb-dc06-41dc-a87b-8802fcd75623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
