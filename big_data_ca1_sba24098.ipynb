{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4990d6bb",
   "metadata": {},
   "source": [
    "# Student Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10880d8f",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Name:** Hoai Nhan Nguyen <br>\n",
    "**Student Number:** sba24098 <br>\n",
    "**Course:** Higher Diploma in Science in Artificial Intelligence Applications\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43ecb5",
   "metadata": {},
   "source": [
    "# Data Cleaning and Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c1c338-b3b0-483a-a0af-7e3c7a19140e",
   "metadata": {},
   "source": [
    "**Importing Apache Spark Libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf6ec5ed-0709-407b-8f98-2b3b8e861513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, sum, when, regexp_replace, round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bfc4d5-0bf6-4c20-bb03-0ca96987dabd",
   "metadata": {},
   "source": [
    "**Creating a new Spark Session.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1404d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31ab9d-7164-44b6-8803-53a7e056981c",
   "metadata": {},
   "source": [
    "**Reading the Amazon-Products.csv in Hadoop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b081a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Reading Amazon-Products.csv in Hadoop while applying options to read it correctly\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"multiLine\", \"true\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe20883-ed09-4423-9ff6-d4eb830a261e",
   "metadata": {},
   "source": [
    "**Understanding the structure of the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc1d3ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- image: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- ratings: string (nullable = true)\n",
      " |-- no_of_ratings: string (nullable = true)\n",
      " |-- discount_price: string (nullable = true)\n",
      " |-- actual_price: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reviewing the schema of the Spark Dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "190750ef-e7ca-4ac0-aab1-8ac1e4baaa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- ratings: string (nullable = true)\n",
      " |-- no_of_ratings: string (nullable = true)\n",
      " |-- discount_price: string (nullable = true)\n",
      " |-- actual_price: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping the columns that are not required for this task \n",
    "df = df.drop(\"_c0\",\"image\",\"link\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79d4831d-aa1f-4c33-9f9d-3e439f9839ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------------+-------+-------------+--------------+------------+\n",
      "|                name|main_category|    sub_category|ratings|no_of_ratings|discount_price|actual_price|\n",
      "+--------------------+-------------+----------------+-------+-------------+--------------+------------+\n",
      "|Lloyd 1.5 Ton 3 S...|   appliances|Air Conditioners|    4.2|        2,255|       ₹32,999|     ₹58,990|\n",
      "|LG 1.5 Ton 5 Star...|   appliances|Air Conditioners|    4.2|        2,948|       ₹46,490|     ₹75,990|\n",
      "|LG 1 Ton 4 Star A...|   appliances|Air Conditioners|    4.2|        1,206|       ₹34,490|     ₹61,990|\n",
      "|LG 1.5 Ton 3 Star...|   appliances|Air Conditioners|    4.0|           69|       ₹37,990|     ₹68,990|\n",
      "|Carrier 1.5 Ton 3...|   appliances|Air Conditioners|    4.1|          630|       ₹34,490|     ₹67,790|\n",
      "|Voltas 1.4 Ton 3 ...|   appliances|Air Conditioners|    4.0|        1,666|       ₹31,990|     ₹70,990|\n",
      "|Lloyd 1.0 Ton 3 S...|   appliances|Air Conditioners|    4.2|        1,097|       ₹29,999|     ₹49,990|\n",
      "|Lloyd 1.5 Ton 5 S...|   appliances|Air Conditioners|    4.3|        1,494|       ₹39,990|     ₹67,990|\n",
      "|Carrier 1 Ton 3 S...|   appliances|Air Conditioners|    4.1|          674|       ₹30,990|     ₹58,190|\n",
      "|Voltas 1.5 Ton, 5...|   appliances|Air Conditioners|    4.0|          801|       ₹37,999|     ₹73,990|\n",
      "|Daikin 1 Ton 3 St...|   appliances|Air Conditioners|    4.2|          558|       ₹32,990|     ₹48,200|\n",
      "|Daikin 1.5 Ton 5 ...|   appliances|Air Conditioners|    4.1|          730|       ₹44,990|     ₹67,200|\n",
      "|Panasonic 1.5 Ton...|   appliances|Air Conditioners|    4.3|        5,073|       ₹45,990|     ₹63,400|\n",
      "|Carrier 1.5 Ton 5...|   appliances|Air Conditioners|    4.0|          568|       ₹41,999|     ₹78,490|\n",
      "|Whirlpool 1.5 Ton...|   appliances|Air Conditioners|    3.9|        3,670|       ₹31,990|     ₹62,000|\n",
      "|Samsung 1.5 Ton 3...|   appliances|Air Conditioners|    3.8|          312|       ₹35,499|     ₹60,990|\n",
      "|Lloyd 1.0 Ton 5 S...|   appliances|Air Conditioners|    4.1|           88|       ₹34,000|     ₹57,990|\n",
      "|Godrej 1.5 Ton 5 ...|   appliances|Air Conditioners|    4.1|          432|       ₹37,490|     ₹54,900|\n",
      "|Godrej 1 Ton 3 St...|   appliances|Air Conditioners|    3.9|          268|       ₹29,490|     ₹42,900|\n",
      "|Daikin 1.5 Ton 3 ...|   appliances|Air Conditioners|    3.5|            2|       ₹39,499|     ₹58,400|\n",
      "+--------------------+-------------+----------------+-------+-------------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 20:33:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      " Schema: _c0, name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products.csv\n"
     ]
    }
   ],
   "source": [
    "# Reviewing the rows of the Spark dataframe \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e910f74-6b7c-4256-8ff6-19b82200b754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 20:33:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      " Schema: _c0, name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products.csv\n",
      "[Stage 14:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (551585, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Get the number of rows\n",
    "num_rows = df.count()\n",
    "# Get the number of columns\n",
    "num_columns = len(df.columns)\n",
    "\n",
    "# Printing the shape (rows, columns)\n",
    "print(f\"Shape of the DataFrame: ({num_rows}, {num_columns})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a620b7-2616-4d8b-9a9c-cd4a43c6be31",
   "metadata": {},
   "source": [
    "**Checking the null values in the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ff8817d-f227-4bca-968f-cc8d452a4af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 20:34:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      " Schema: _c0, name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products.csv\n",
      "[Stage 23:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+------------+-------+-------------+--------------+------------+\n",
      "|name|main_category|sub_category|ratings|no_of_ratings|discount_price|actual_price|\n",
      "+----+-------------+------------+-------+-------------+--------------+------------+\n",
      "|   0|            0|           0| 175794|       175794|         61163|       17813|\n",
      "+----+-------------+------------+-------+-------------+--------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Counting the null or empty values per column\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a6afc-0c41-4cc2-92cb-9ffae6040a0c",
   "metadata": {},
   "source": [
    "**Handling Null values in the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a3d40-cf86-4065-9344-16ddcbc5d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out rows where both 'discount_price' and 'actual_price' are null.\n",
    "df_clean = df.filter(~(col(\"discount_price\").isNull() & col(\"actual_price\").isNull()))\n",
    "\n",
    "# Filling the null values for ratings, no_of_ratings, discount_price and actual_price to 0\n",
    "df_clean = df_clean.fillna({\"ratings\": 0,\"no_of_ratings\": 0, \"discount_price\":0, \"actual_price\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7842589-d38f-4377-9a38-665f8e74d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the null or empty values per column\n",
    "df_clean.select([\n",
    "    sum(when(col(column_name).isNull() | (col(column_name) == \"\"), 1).otherwise(0)).alias(column_name + \"_nulls\")\n",
    "    for column_name in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5c46e-c03d-427f-aca6-43a0dffd90e3",
   "metadata": {},
   "source": [
    "**Handling duplicate rows in the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc40f86-5430-466d-8c6e-1a93e8197bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the duplicate rows from the Spark DataFrame \n",
    "df_clean = df_clean.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c2703-939f-41d3-b083-20505a8240cd",
   "metadata": {},
   "source": [
    "**Cleaning the ratings and no_of_rating columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e79a28-64d3-4802-af5b-4a2a031999a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring the rows where the 'ratings' column contains valid numbers (integers or decimals).\n",
    "df_clean = df_clean.filter(F.col('ratings').rlike(r'^[0-9]*\\.?[0-9]+$'))\n",
    "\n",
    "# Ensuring ratings are between 0 and 5.0\n",
    "df_clean = df_clean.filter((F.col('ratings') >= 0) & (F.col('ratings') <= 5.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ddbc4-00de-4a0d-930e-d5ccf55bcdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing commas from 'no_of_ratings'\n",
    "df_clean = df_clean.withColumn(\"no_of_ratings\", regexp_replace(col(\"no_of_ratings\"), \",\", \"\"))\n",
    "\n",
    "# Ensuring the rows where the 'no_of_ratings' column contains valid numbers (integers).\n",
    "df_clean = df_clean.filter(col(\"no_of_ratings\").rlike(\"^[0-9]+$\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6891ecd-f221-4a00-b1a4-38ab18425105",
   "metadata": {},
   "source": [
    "**Converting currency from Indian Rupee to Euro for the actual_price and discount_price columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e0b0fe-9222-4273-b362-b5b2d3ad90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing ₹, commas and convert to double\n",
    "df_converted = df_clean.withColumn(\n",
    "    \"actual_price\",\n",
    "    regexp_replace(col(\"actual_price\"), \"[₹,]\", \"\").cast(\"double\")  \n",
    ")\n",
    "\n",
    "# Converting INR to EUR (using conversion rate: 1 INR = 0.011 EUR)\n",
    "conversion_rate = 0.011\n",
    "df_converted = df_converted.withColumn(\n",
    "    \"actual_price\",\n",
    "    round(col(\"actual_price\") * conversion_rate, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1ee81-40cb-4b07-934f-57e5754a2ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing ₹, commas and convert to double\n",
    "df_converted = df_converted.withColumn(\n",
    "    \"discount_price\",\n",
    "    regexp_replace(col(\"discount_price\"), \"[₹,]\", \"\").cast(\"double\") \n",
    ")\n",
    "\n",
    "# Converting INR to EUR (using conversion rate: 1 INR = 0.011 EUR)\n",
    "conversion_rate = 0.011\n",
    "df_converted = df_converted.withColumn(\n",
    "    \"discount_price\",\n",
    "    round(col(\"discount_price\") * conversion_rate, 2) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef09a99-0856-450a-9d08-ab4f6c9cf002",
   "metadata": {},
   "source": [
    "**Saving converted Spark Dataframe as a CSV file in Hadoop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250822df-52a7-4300-84c7-718c872905f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_converted.write \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"quoteAll\", \"true\") \\\n",
    "  .option(\"escape\", \"\\\"\") \\\n",
    "  .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products-Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86106ac7-3122-4fa0-b90d-5a7eae9ac16e",
   "metadata": {},
   "source": [
    "# Inserting CSV Data into HBase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb656d-c4c5-4619-b160-f96963373cbc",
   "metadata": {},
   "source": [
    "**Importing HappyBase Library to connect to HBase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f868b-c085-4df3-9083-7f488ba260c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import happybase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5743b17-f720-4ff0-aa55-1ba370375ca5",
   "metadata": {},
   "source": [
    "**Reading the Amazon-Products-Cleaned.csv in Hadoop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f825f46-cb20-4e32-a0a2-e6a63467669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Amazon-Products-Cleaned.csv in Hadoop while applying options to read it correctly\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"multiLine\", \"true\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products-Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e487ab-1c87-4dd8-be7d-80b6489f02a7",
   "metadata": {},
   "source": [
    "**Reviewing the Schema of Amazon-Products-Cleaned.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b55f6f2-9b9e-4fef-be72-09bbe55aa2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- ratings: double (nullable = true)\n",
      " |-- no_of_ratings: integer (nullable = true)\n",
      " |-- discount_price: double (nullable = true)\n",
      " |-- actual_price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reviewing the schema of the Spark Dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c84a6-bac1-4a94-b08a-191d56d60d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9e6b1-6a06-405a-bcde-2f68bcfc3bb1",
   "metadata": {},
   "source": [
    "**Connecting to Hbase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19318bb4-090c-4bcc-9c8f-3ed4d8e1f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to HBase with the happybase library \n",
    "connection = happybase.Connection('localhost')\n",
    "connection.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab3ef53-e14d-4296-b105-a335bf948ab3",
   "metadata": {},
   "source": [
    "**Creating and adding data to the table.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034cc815-9aca-4ae9-88ad-c18ecf55d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the column families \n",
    "column_families = {\n",
    "    'Item_Info': dict(),\n",
    "    'Ratings_Info': dict(),\n",
    "    'Pricing_Info': dict(),\n",
    "}\n",
    "\n",
    "# Creating the table amazon_products if it doesn't exist\n",
    "table_name = 'amazon_products'\n",
    "if table_name not in connection.tables():\n",
    "    connection.create_table(table_name, column_families)\n",
    "\n",
    "# Getting the table amazon_products\n",
    "table = connection.table(table_name)\n",
    "\n",
    "# For loop to add the data from Amazon-Products-Cleaned.csv to the table amazon_products\n",
    "for idx, row in enumerate(df.rdd.collect(), start=1):\n",
    "    # Generating a 6-character row key for each item\n",
    "    row_key = str(idx).zfill(6)\n",
    "    \n",
    "    # Adding the data to the table\n",
    "    table.put(row_key, {\n",
    "        'Item_Info:name': row['name'],\n",
    "        'Item_Info:main_category': row['main_category'],\n",
    "        'Item_Info:sub_category': row['sub_category'],\n",
    "        'Ratings_Info:ratings': str(row['ratings']),\n",
    "        'Ratings_Info:no_of_ratings': str(row['no_of_ratings']),\n",
    "        'Pricing_Info:discount_price': str(row['discount_price']),\n",
    "        'Pricing_Info:actual_price': str(row['actual_price'])\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a484c72-6edb-476e-9e43-dc0c8dd44540",
   "metadata": {},
   "source": [
    "**Closing the connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69038c3c-cd91-45b3-9e4f-54c39f4742dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d726593-3995-4aaf-b8ad-818fadebebc9",
   "metadata": {},
   "source": [
    "# Apache Spark - Basic Analysis and Insights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75717140-7c26-4211-a56d-7fa501dc5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Amazon-Products-Cleaned.csv in Hadoop while applying options to read it correctly\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"multiLine\", \"true\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products-Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45dfa7a9-9dc2-4d59-969e-e19211ec12de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------+-------------+--------------+------------+\n",
      "|                name|       main_category|        sub_category|ratings|no_of_ratings|discount_price|actual_price|\n",
      "+--------------------+--------------------+--------------------+-------+-------------+--------------+------------+\n",
      "|Pampers Swaddlers...|toys & baby products|             Diapers|    4.9|        26160|        143.09|      204.48|\n",
      "|Medela Breastmilk...|toys & baby products|   Nursing & Feeding|    4.9|         7404|         52.95|       133.1|\n",
      "|Pampers Diapers S...|toys & baby products|             Diapers|    4.9|         6553|        220.71|       607.2|\n",
      "|DOWAN multi color...|      home & kitchen|    Kitchen & Dining|    4.9|         5485|          4.94|       54.99|\n",
      "|Scrub Daddy Insta...|          appliances|Kitchen & Home Ap...|    4.9|         2285|         15.39|        38.5|\n",
      "|Scrub Daddy Insta...|          appliances|      All Appliances|    4.9|         2285|         15.39|        38.5|\n",
      "|Scrub Daddy Insta...|          appliances|Heating & Cooling...|    4.9|         2285|         15.39|        38.5|\n",
      "|A&D Ointment, Dia...|toys & baby products|             Diapers|    4.9|         1853|           0.0|       94.36|\n",
      "|Psalm 46: 10 - Pu...|              stores|       Men's Fashion|    4.9|         1158|           0.0|       32.21|\n",
      "|Ravensburger Puzz...|toys & baby products|International Toy...|    4.9|         1012|           0.0|       39.97|\n",
      "+--------------------+--------------------+--------------------+-------+-------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering dataframe for the top 10 for ratings equal or more than 4.5 and number of ratings more than 1000.\n",
    "df.filter((F.col(\"ratings\") >= 4.5) & (F.col(\"no_of_ratings\") > 1000)) \\\n",
    "  .orderBy(F.desc(\"ratings\"), F.desc(\"no_of_ratings\")) \\\n",
    "  .limit(10) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f39b9c14-f8e4-4ed1-876a-bc88ce906d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+-------+-------------+--------------+------------+\n",
      "|                name|      main_category|        sub_category|ratings|no_of_ratings|discount_price|actual_price|\n",
      "+--------------------+-------------------+--------------------+-------+-------------+--------------+------------+\n",
      "|DEVCOMM Phone Blu...|tv, audio & cameras|     All Electronics|    2.5|         1225|          1.54|         3.3|\n",
      "|Prestige Roti Mak...|         appliances|      All Appliances|    2.6|         1411|         31.61|       37.35|\n",
      "|Prestige Roti Mak...|         appliances|Kitchen & Home Ap...|    2.6|         1411|         31.61|       37.35|\n",
      "|TYING Men's Trend...|        men's shoes|        Casual Shoes|    2.7|         1942|          3.29|       10.98|\n",
      "|Kuber Industries ...|     home & kitchen|  All Home & Kitchen|    2.7|         1977|          3.83|        5.49|\n",
      "|Generic Silica Ge...|    beauty & health|             Make-up|    2.8|         1645|          1.74|        3.29|\n",
      "|Amrange Non Woven...|industrial supplies|Industrial & Scie...|    2.8|         3981|          3.29|       10.99|\n",
      "|NEOSAFE Spark A50...|             stores|       Men's Fashion|    2.8|         6942|          4.23|        8.47|\n",
      "|BLACK MACY Women ...|   women's clothing|         Ethnic Wear|    2.9|         1128|          8.79|       10.99|\n",
      "|BLACK MACY Women ...|   women's clothing|            Clothing|    2.9|         1128|          8.79|       10.99|\n",
      "+--------------------+-------------------+--------------------+-------+-------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering dataframe for the bottom 10 for ratings equal or more than 1 and number of ratings more than 1000.\n",
    "df.filter((F.col(\"ratings\") >= 1) & (F.col(\"no_of_ratings\") > 1000)) \\\n",
    "  .orderBy(F.asc(\"ratings\"), F.asc(\"no_of_ratings\")) \\\n",
    "  .limit(10) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "25cf589a-7936-454a-8c50-51b5c06e8a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+----------------+----------------+------------------+------------------+------------------+\n",
      "|       main_category|min_actual_price|max_actual_price|avg_actual_price|min_discount_price|max_discount_price|avg_discount_price|\n",
      "+--------------------+----------------+----------------+----------------+------------------+------------------+------------------+\n",
      "|    women's clothing|            0.65|          549.99|           19.39|              0.00|            252.99|              6.69|\n",
      "|         men's shoes|            0.43|          734.57|           42.23|              0.00|            733.14|             16.92|\n",
      "|toys & baby products|            0.16|          630.27|           20.80|              0.00|            332.56|              9.58|\n",
      "| home, kitchen, pets|           22.00|           87.56|           52.14|             19.80|             55.44|             34.89|\n",
      "|          appliances|            0.65|        6,600.00|           90.30|              0.00|          2,530.00|             56.90|\n",
      "|     beauty & health|            0.11|        1,320.00|           12.63|              0.00|          1,045.00|              6.82|\n",
      "| tv, audio & cameras|            0.00|       17,543.90|           63.15|              0.00|         13,749.89|             35.25|\n",
      "|      men's clothing|            0.33|          445.46|           19.46|              0.00|            284.13|              8.92|\n",
      "|      home & kitchen|            0.27|  108,899,999.99|        7,797.81|              0.00|            665.50|              9.66|\n",
      "|               music|            0.66|        1,346.84|           62.15|              0.00|            845.90|             38.78|\n",
      "|        pet supplies|            0.44|          297.80|           12.54|              0.00|            163.79|              6.96|\n",
      "| industrial supplies|            0.59|        1,045.00|           36.10|              0.00|            614.35|             20.63|\n",
      "|     car & motorbike|            0.36|        2,089.89|           19.40|              0.00|          1,318.90|              9.91|\n",
      "|         accessories|            0.04|       13,200.00|           84.89|              0.00|          5,499.99|             55.90|\n",
      "|       kids' fashion|            1.03|       13,200.00|           18.46|              0.00|          4,466.10|              8.12|\n",
      "|      bags & luggage|            0.88|        2,139.49|           42.92|              0.00|          1,540.00|             18.55|\n",
      "|grocery & gourmet...|            0.11|          418.00|            6.38|              0.00|            152.89|              3.85|\n",
      "|              stores|            0.00|       13,200.00|           56.02|              0.00|          4,466.10|             11.61|\n",
      "|       women's shoes|            1.75|          494.85|           22.82|              0.00|            166.08|              9.48|\n",
      "|    sports & fitness|            0.55|      671,912.89|           91.09|              0.00|          2,477.75|             17.59|\n",
      "+--------------------+----------------+----------------+----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping by 'main_category' and calculate summary statistics for prices\n",
    "df_price_stats = df.groupBy(\"main_category\").agg(\n",
    "    F.format_number(F.min(\"actual_price\"), 2).alias(\"min_actual_price\"),\n",
    "    F.format_number(F.max(\"actual_price\"), 2).alias(\"max_actual_price\"),\n",
    "    F.format_number(F.avg(\"actual_price\"), 2).alias(\"avg_actual_price\"),\n",
    "    F.format_number(F.min(\"discount_price\"), 2).alias(\"min_discount_price\"),\n",
    "    F.format_number(F.max(\"discount_price\"), 2).alias(\"max_discount_price\"),\n",
    "    F.format_number(F.avg(\"discount_price\"), 2).alias(\"avg_discount_price\"),\n",
    ")\n",
    "\n",
    "# Show dataframe\n",
    "df_price_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7fd8268f-a574-400e-957c-3154fd3a643d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-------------------+\n",
      "|total_actual_price|total_discount_price|total_discount_loss|\n",
      "+------------------+--------------------+-------------------+\n",
      "|    133,376,837.61|       12,885,616.45|     120,491,221.16|\n",
      "+------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the total discount loss based on actual price - discount price\n",
    "df.withColumn(\"discount_loss\", F.col(\"actual_price\") - F.col(\"discount_price\")) \\\n",
    "  .agg(\n",
    "      F.format_number(F.sum(\"actual_price\"), 2).alias(\"total_actual_price\"),\n",
    "      F.format_number(F.sum(\"discount_price\"), 2).alias(\"total_discount_price\"),\n",
    "      F.format_number(F.sum(\"discount_loss\"), 2).alias(\"total_discount_loss\")\n",
    "  ) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc7f8114-3f07-4bd6-b9f7-2e5eb93cf35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+\n",
      "|       main_category|average_rating|total_number_ratings|\n",
      "+--------------------+--------------+--------------------+\n",
      "| home, kitchen, pets|           0.0|                   0|\n",
      "|               music|           3.2|              283319|\n",
      "|       women's shoes|           2.0|              547897|\n",
      "|        pet supplies|           3.6|              959921|\n",
      "| industrial supplies|           2.8|             1050115|\n",
      "|      bags & luggage|           1.6|             1365586|\n",
      "|     car & motorbike|           3.0|             1498404|\n",
      "|grocery & gourmet...|           3.7|             1635936|\n",
      "|       kids' fashion|           1.9|             1679459|\n",
      "|    sports & fitness|           2.8|             2784355|\n",
      "|      men's clothing|           2.2|             5093413|\n",
      "|         men's shoes|           2.0|             5805636|\n",
      "|      home & kitchen|           3.7|             9319642|\n",
      "|     beauty & health|           3.2|            10213857|\n",
      "|         accessories|           2.4|            11031399|\n",
      "|toys & baby products|           3.5|            13513920|\n",
      "|    women's clothing|           3.2|            13596870|\n",
      "|          appliances|           3.3|            17586516|\n",
      "|              stores|           3.5|            19635285|\n",
      "| tv, audio & cameras|           2.8|           169455971|\n",
      "+--------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping by 'main_category' and calculate average rating and total number of ratings\n",
    "df.groupBy(\"main_category\") \\\n",
    "  .agg(\n",
    "      F.round(F.avg(\"ratings\"), 1).alias(\"average_rating\"),\n",
    "      F.sum(\"no_of_ratings\").alias(\"total_number_ratings\")\n",
    "  ) \\\n",
    "  .orderBy(\"total_number_ratings\") \\\n",
    "  .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
