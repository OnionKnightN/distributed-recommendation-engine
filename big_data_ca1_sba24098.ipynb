{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4990d6bb",
   "metadata": {},
   "source": [
    "# Student Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10880d8f",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Name:** Hoai Nhan Nguyen <br>\n",
    "**Student Number:** sba24098 <br>\n",
    "**Course:** Higher Diploma in Science in Artificial Intelligence Applications\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43ecb5",
   "metadata": {},
   "source": [
    "# Data Cleaning and Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c1c338-b3b0-483a-a0af-7e3c7a19140e",
   "metadata": {},
   "source": [
    "**Importing Apache Spark Libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6ec5ed-0709-407b-8f98-2b3b8e861513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, sum, when, regexp_replace, round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bfc4d5-0bf6-4c20-bb03-0ca96987dabd",
   "metadata": {},
   "source": [
    "**Creating a new Spark Session.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1404d5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 23:24:35 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# Creating new SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31ab9d-7164-44b6-8803-53a7e056981c",
   "metadata": {},
   "source": [
    "**Reading the Amazon-Products.csv in Hadoop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b081a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Reading Amazon-Products.csv in Hadoop while applying options to read it correctly\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"multiLine\", \"true\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe20883-ed09-4423-9ff6-d4eb830a261e",
   "metadata": {},
   "source": [
    "**Understanding the structure of the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc1d3ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- image: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- ratings: string (nullable = true)\n",
      " |-- no_of_ratings: string (nullable = true)\n",
      " |-- discount_price: string (nullable = true)\n",
      " |-- actual_price: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reviewing the schema of the Spark Dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "190750ef-e7ca-4ac0-aab1-8ac1e4baaa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- ratings: string (nullable = true)\n",
      " |-- no_of_ratings: string (nullable = true)\n",
      " |-- discount_price: string (nullable = true)\n",
      " |-- actual_price: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping the columns that are not required for this task \n",
    "df = df.drop(\"_c0\",\"image\",\"link\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79d4831d-aa1f-4c33-9f9d-3e439f9839ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------------+-------+-------------+--------------+------------+\n",
      "|                name|main_category|    sub_category|ratings|no_of_ratings|discount_price|actual_price|\n",
      "+--------------------+-------------+----------------+-------+-------------+--------------+------------+\n",
      "|Lloyd 1.5 Ton 3 S...|   appliances|Air Conditioners|    4.2|        2,255|       ₹32,999|     ₹58,990|\n",
      "|LG 1.5 Ton 5 Star...|   appliances|Air Conditioners|    4.2|        2,948|       ₹46,490|     ₹75,990|\n",
      "|LG 1 Ton 4 Star A...|   appliances|Air Conditioners|    4.2|        1,206|       ₹34,490|     ₹61,990|\n",
      "|LG 1.5 Ton 3 Star...|   appliances|Air Conditioners|    4.0|           69|       ₹37,990|     ₹68,990|\n",
      "|Carrier 1.5 Ton 3...|   appliances|Air Conditioners|    4.1|          630|       ₹34,490|     ₹67,790|\n",
      "|Voltas 1.4 Ton 3 ...|   appliances|Air Conditioners|    4.0|        1,666|       ₹31,990|     ₹70,990|\n",
      "|Lloyd 1.0 Ton 3 S...|   appliances|Air Conditioners|    4.2|        1,097|       ₹29,999|     ₹49,990|\n",
      "|Lloyd 1.5 Ton 5 S...|   appliances|Air Conditioners|    4.3|        1,494|       ₹39,990|     ₹67,990|\n",
      "|Carrier 1 Ton 3 S...|   appliances|Air Conditioners|    4.1|          674|       ₹30,990|     ₹58,190|\n",
      "|Voltas 1.5 Ton, 5...|   appliances|Air Conditioners|    4.0|          801|       ₹37,999|     ₹73,990|\n",
      "|Daikin 1 Ton 3 St...|   appliances|Air Conditioners|    4.2|          558|       ₹32,990|     ₹48,200|\n",
      "|Daikin 1.5 Ton 5 ...|   appliances|Air Conditioners|    4.1|          730|       ₹44,990|     ₹67,200|\n",
      "|Panasonic 1.5 Ton...|   appliances|Air Conditioners|    4.3|        5,073|       ₹45,990|     ₹63,400|\n",
      "|Carrier 1.5 Ton 5...|   appliances|Air Conditioners|    4.0|          568|       ₹41,999|     ₹78,490|\n",
      "|Whirlpool 1.5 Ton...|   appliances|Air Conditioners|    3.9|        3,670|       ₹31,990|     ₹62,000|\n",
      "|Samsung 1.5 Ton 3...|   appliances|Air Conditioners|    3.8|          312|       ₹35,499|     ₹60,990|\n",
      "|Lloyd 1.0 Ton 5 S...|   appliances|Air Conditioners|    4.1|           88|       ₹34,000|     ₹57,990|\n",
      "|Godrej 1.5 Ton 5 ...|   appliances|Air Conditioners|    4.1|          432|       ₹37,490|     ₹54,900|\n",
      "|Godrej 1 Ton 3 St...|   appliances|Air Conditioners|    3.9|          268|       ₹29,490|     ₹42,900|\n",
      "|Daikin 1.5 Ton 3 ...|   appliances|Air Conditioners|    3.5|            2|       ₹39,499|     ₹58,400|\n",
      "+--------------------+-------------+----------------+-------+-------------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 23:24:40 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      " Schema: _c0, name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products.csv\n"
     ]
    }
   ],
   "source": [
    "# Reviewing the rows of the Spark dataframe \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e910f74-6b7c-4256-8ff6-19b82200b754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 23:24:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      " Schema: _c0, name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products.csv\n",
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (551585, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Get the number of rows\n",
    "num_rows = df.count()\n",
    "# Get the number of columns\n",
    "num_columns = len(df.columns)\n",
    "\n",
    "# Printing the shape (rows, columns)\n",
    "print(f\"Shape of the DataFrame: ({num_rows}, {num_columns})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a620b7-2616-4d8b-9a9c-cd4a43c6be31",
   "metadata": {},
   "source": [
    "**Checking the null values in the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ff8817d-f227-4bca-968f-cc8d452a4af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 23:24:42 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      " Schema: _c0, name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products.csv\n",
      "[Stage 6:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+------------+-------+-------------+--------------+------------+\n",
      "|name|main_category|sub_category|ratings|no_of_ratings|discount_price|actual_price|\n",
      "+----+-------------+------------+-------+-------------+--------------+------------+\n",
      "|   0|            0|           0| 175794|       175794|         61163|       17813|\n",
      "+----+-------------+------------+-------+-------------+--------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Counting the null or empty values per column\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a6afc-0c41-4cc2-92cb-9ffae6040a0c",
   "metadata": {},
   "source": [
    "**Handling Null values in the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "777a3d40-cf86-4065-9344-16ddcbc5d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out rows where both 'discount_price' and 'actual_price' are null.\n",
    "df_clean = df.filter(~(col(\"discount_price\").isNull() & col(\"actual_price\").isNull()))\n",
    "\n",
    "# Filling the null values for ratings, no_of_ratings, discount_price and actual_price to 0\n",
    "df_clean = df_clean.fillna({\"ratings\": 0,\"no_of_ratings\": 0, \"discount_price\":0, \"actual_price\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7842589-d38f-4377-9a38-665f8e74d14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 23:24:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      " Schema: _c0, name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products.csv\n",
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------------+-------------+-------------------+--------------------+------------------+\n",
      "|name_nulls|main_category_nulls|sub_category_nulls|ratings_nulls|no_of_ratings_nulls|discount_price_nulls|actual_price_nulls|\n",
      "+----------+-------------------+------------------+-------------+-------------------+--------------------+------------------+\n",
      "|         0|                  0|                 0|            0|                  0|                   0|                 0|\n",
      "+----------+-------------------+------------------+-------------+-------------------+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Counting the null or empty values per column\n",
    "df_clean.select([\n",
    "    sum(when(col(column_name).isNull() | (col(column_name) == \"\"), 1).otherwise(0)).alias(column_name + \"_nulls\")\n",
    "    for column_name in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5c46e-c03d-427f-aca6-43a0dffd90e3",
   "metadata": {},
   "source": [
    "**Handling duplicate rows in the Spark Dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbc40f86-5430-466d-8c6e-1a93e8197bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the duplicate rows based on name from the Spark DataFrame \n",
    "df_clean = df_clean.dropDuplicates([\"name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c2703-939f-41d3-b083-20505a8240cd",
   "metadata": {},
   "source": [
    "**Cleaning the ratings and no_of_rating columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57e79a28-64d3-4802-af5b-4a2a031999a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring the rows where the 'ratings' column contains valid numbers (integers or decimals).\n",
    "df_clean = df_clean.filter(F.col('ratings').rlike(r'^[0-9]*\\.?[0-9]+$'))\n",
    "\n",
    "# Ensuring ratings are between 0 and 5.0\n",
    "df_clean = df_clean.filter((F.col('ratings') >= 0) & (F.col('ratings') <= 5.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d5ddbc4-00de-4a0d-930e-d5ccf55bcdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing commas from 'no_of_ratings'\n",
    "df_clean = df_clean.withColumn(\"no_of_ratings\", regexp_replace(col(\"no_of_ratings\"), \",\", \"\"))\n",
    "\n",
    "# Ensuring the rows where the 'no_of_ratings' column contains valid numbers (integers).\n",
    "df_clean = df_clean.filter(col(\"no_of_ratings\").rlike(\"^[0-9]+$\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6891ecd-f221-4a00-b1a4-38ab18425105",
   "metadata": {},
   "source": [
    "**Converting currency from Indian Rupee to Euro for the actual_price and discount_price columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e0b0fe-9222-4273-b362-b5b2d3ad90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing ₹, commas and convert to double\n",
    "df_converted = df_clean.withColumn(\n",
    "    \"actual_price\",\n",
    "    regexp_replace(col(\"actual_price\"), \"[₹,]\", \"\").cast(\"double\")  \n",
    ")\n",
    "\n",
    "# Converting INR to EUR (using conversion rate: 1 INR = 0.011 EUR)\n",
    "conversion_rate = 0.011\n",
    "df_converted = df_converted.withColumn(\n",
    "    \"actual_price\",\n",
    "    round(col(\"actual_price\") * conversion_rate, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbe1ee81-40cb-4b07-934f-57e5754a2ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing ₹, commas and convert to double\n",
    "df_converted = df_converted.withColumn(\n",
    "    \"discount_price\",\n",
    "    regexp_replace(col(\"discount_price\"), \"[₹,]\", \"\").cast(\"double\") \n",
    ")\n",
    "\n",
    "# Converting INR to EUR (using conversion rate: 1 INR = 0.011 EUR)\n",
    "conversion_rate = 0.011\n",
    "df_converted = df_converted.withColumn(\n",
    "    \"discount_price\",\n",
    "    round(col(\"discount_price\") * conversion_rate, 2) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef09a99-0856-450a-9d08-ab4f6c9cf002",
   "metadata": {},
   "source": [
    "**Saving converted Spark Dataframe as a CSV file in Hadoop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "250822df-52a7-4300-84c7-718c872905f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 23:24:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      " Schema: _c0, name, main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products.csv\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_converted.write \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"quoteAll\", \"true\") \\\n",
    "  .option(\"escape\", \"\\\"\") \\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products-Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86106ac7-3122-4fa0-b90d-5a7eae9ac16e",
   "metadata": {},
   "source": [
    "# Inserting CSV Data into HBase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb656d-c4c5-4619-b160-f96963373cbc",
   "metadata": {},
   "source": [
    "**Importing HappyBase Library to connect to HBase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc8f868b-c085-4df3-9083-7f488ba260c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import happybase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5743b17-f720-4ff0-aa55-1ba370375ca5",
   "metadata": {},
   "source": [
    "**Reading the Amazon-Products-Cleaned.csv in Hadoop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f825f46-cb20-4e32-a0a2-e6a63467669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Amazon-Products-Cleaned.csv in Hadoop while applying options to read it correctly\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"multiLine\", \"true\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products-Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e487ab-1c87-4dd8-be7d-80b6489f02a7",
   "metadata": {},
   "source": [
    "**Reviewing the Schema of Amazon-Products-Cleaned.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b55f6f2-9b9e-4fef-be72-09bbe55aa2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- ratings: double (nullable = true)\n",
      " |-- no_of_ratings: integer (nullable = true)\n",
      " |-- discount_price: double (nullable = true)\n",
      " |-- actual_price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reviewing the schema of the Spark Dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa4c84a6-bac1-4a94-b08a-191d56d60d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+-------+-------------+--------------+------------+\n",
      "|                name|      main_category|        sub_category|ratings|no_of_ratings|discount_price|actual_price|\n",
      "+--------------------+-------------------+--------------------+-------+-------------+--------------+------------+\n",
      "|\"DN Enterprises\" ...|        accessories| Handbags & Clutches|    3.4|           24|          3.84|       14.29|\n",
      "|\"GOMUKH” Gangajal...|    beauty & health|  Household Supplies|    4.8|            5|          3.62|        4.39|\n",
      "|\"Handicraft-Palac...|   women's clothing|Lingerie & Nightwear|    4.0|            3|          8.24|       10.99|\n",
      "|\"Jai Guru Ji |Gur...|        accessories|           Jewellery|    0.0|            0|          2.63|        3.29|\n",
      "|\"Ji\" Japsin Instr...|industrial supplies|Test, Measure & I...|    4.5|            5|          23.1|        33.0|\n",
      "|\"Ji\" Japsin Instr...|industrial supplies|    Lab & Scientific|    4.2|            3|          5.01|        7.15|\n",
      "|\"Ji\" Japsin Instr...|industrial supplies|Test, Measure & I...|    5.0|            1|          6.55|        9.24|\n",
      "|\"Ji\" Japsin Instr...|industrial supplies|Test, Measure & I...|    0.0|            0|          8.69|       17.38|\n",
      "|\"Ji\" Japsin Instr...|industrial supplies|    Lab & Scientific|    0.0|            0|         13.86|        19.8|\n",
      "|\"Ji\" Japsin Instr...|industrial supplies|Test, Measure & I...|    0.0|            0|         14.17|       31.89|\n",
      "|\"PH\" POSHAKHUB Wo...|   women's clothing|         Ethnic Wear|    3.4|           35|          9.89|       12.09|\n",
      "|\"PH\" POSHAKHUB Wo...|   women's clothing|         Ethnic Wear|    4.0|          235|         10.39|       11.54|\n",
      "|\"Plant Daddy\" Suc...|        accessories|           Jewellery|    0.0|            0|          3.29|        9.34|\n",
      "|\"SRS Trends\" Larg...|        accessories|      Bags & Luggage|    3.8|           13|          3.29|        5.49|\n",
      "|\"TGC\" Black Colou...|        accessories|Fashion & Silver ...|    3.2|            2|          1.75|        8.79|\n",
      "|\"TGC\" Light Maroo...|        accessories|Fashion & Silver ...|    4.4|            3|          1.75|        5.49|\n",
      "|\"TGC\" Light Purpl...|        accessories|Fashion & Silver ...|    2.5|            4|          1.75|        7.69|\n",
      "|\"TGC\" Mango Yello...|        accessories|Fashion & Silver ...|    4.6|            7|          1.75|        4.39|\n",
      "|\"TGC\" Purple Colo...|        accessories|Fashion & Silver ...|    5.0|            1|          1.75|        7.69|\n",
      "|\"TGC\" Rani Colour...|        accessories|Fashion & Silver ...|    4.0|           12|          1.75|        5.49|\n",
      "+--------------------+-------------------+--------------------+-------+-------------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9e6b1-6a06-405a-bcde-2f68bcfc3bb1",
   "metadata": {},
   "source": [
    "**Connecting to Hbase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19318bb4-090c-4bcc-9c8f-3ed4d8e1f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to HBase with the happybase library \n",
    "connection = happybase.Connection('localhost')\n",
    "connection.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab3ef53-e14d-4296-b105-a335bf948ab3",
   "metadata": {},
   "source": [
    "**Creating and adding data to the table.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "034cc815-9aca-4ae9-88ad-c18ecf55d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Defining the column families \n",
    "column_families = {\n",
    "    'Item_Info': dict(),\n",
    "    'Ratings_Info': dict(),\n",
    "    'Pricing_Info': dict(),\n",
    "}\n",
    "\n",
    "# Creating the table amazon_products if it doesn't exist\n",
    "table_name = 'amazon_products'\n",
    "if table_name not in connection.tables():\n",
    "    connection.create_table(table_name, column_families)\n",
    "\n",
    "# Getting the table amazon_products\n",
    "table = connection.table(table_name)\n",
    "\n",
    "# For loop to add the data from Amazon-Products-Cleaned.csv to the table amazon_products\n",
    "for idx, row in enumerate(df.rdd.collect(), start=1):\n",
    "    # Generating a 6-character row key for each item\n",
    "    row_key = str(idx).zfill(6)\n",
    "    \n",
    "    # Adding the data to the table\n",
    "    table.put(row_key, {\n",
    "        'Item_Info:name': row['name'],\n",
    "        'Item_Info:main_category': row['main_category'],\n",
    "        'Item_Info:sub_category': row['sub_category'],\n",
    "        'Ratings_Info:ratings': str(row['ratings']),\n",
    "        'Ratings_Info:no_of_ratings': str(row['no_of_ratings']),\n",
    "        'Pricing_Info:discount_price': str(row['discount_price']),\n",
    "        'Pricing_Info:actual_price': str(row['actual_price'])\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a484c72-6edb-476e-9e43-dc0c8dd44540",
   "metadata": {},
   "source": [
    "**Closing the connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69038c3c-cd91-45b3-9e4f-54c39f4742dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d726593-3995-4aaf-b8ad-818fadebebc9",
   "metadata": {},
   "source": [
    "# Apache Spark - Basic Analysis and Insights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b30ddacd-b4b7-4bc9-bff7-fbbd8cd594e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75717140-7c26-4211-a56d-7fa501dc5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Amazon-Products-Cleaned.csv in Hadoop while applying options to read it correctly\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"multiLine\", \"true\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .csv(\"hdfs://localhost:9000/user1/big_data_ca1/data/Amazon-Products-Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45dfa7a9-9dc2-4d59-969e-e19211ec12de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+-------+-------------+--------------+------------+\n",
      "|                name|       main_category|     sub_category|ratings|no_of_ratings|discount_price|actual_price|\n",
      "+--------------------+--------------------+-----------------+-------+-------------+--------------+------------+\n",
      "|Pampers Swaddlers...|toys & baby products|          Diapers|    4.9|        26160|        143.09|      204.48|\n",
      "|Medela Breastmilk...|toys & baby products|Nursing & Feeding|    4.9|         7404|         52.95|       133.1|\n",
      "|Pampers Diapers S...|toys & baby products|          Diapers|    4.9|         6553|        220.71|       607.2|\n",
      "|DOWAN multi color...|      home & kitchen| Kitchen & Dining|    4.9|         5485|          4.94|       54.99|\n",
      "|Scrub Daddy Insta...|          appliances|   All Appliances|    4.9|         2285|         15.39|        38.5|\n",
      "+--------------------+--------------------+-----------------+-------+-------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataframe for ratings >= 4.5 and number of ratings > 1000\n",
    "df.filter((F.col(\"ratings\") >= 4.5) & (F.col(\"no_of_ratings\") > 1000)) \\\n",
    "  .orderBy(F.desc(\"ratings\"), F.desc(\"no_of_ratings\")) \\\n",
    "  .limit(5)\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f39b9c14-f8e4-4ed1-876a-bc88ce906d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------------+-------+-------------+--------------+------------+\n",
      "|                name|      main_category|      sub_category|ratings|no_of_ratings|discount_price|actual_price|\n",
      "+--------------------+-------------------+------------------+-------+-------------+--------------+------------+\n",
      "|DEVCOMM Phone Blu...|tv, audio & cameras|   All Electronics|    2.5|         1225|          1.54|         3.3|\n",
      "|Prestige Roti Mak...|         appliances|    All Appliances|    2.6|         1411|         31.61|       37.35|\n",
      "|TYING Men's Trend...|        men's shoes|      Casual Shoes|    2.7|         1942|          3.29|       10.98|\n",
      "|Kuber Industries ...|     home & kitchen|All Home & Kitchen|    2.7|         1977|          3.83|        5.49|\n",
      "|Generic Silica Ge...|    beauty & health|           Make-up|    2.8|         1645|          1.74|        3.29|\n",
      "+--------------------+-------------------+------------------+-------+-------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering dataframe for the bottom 10 for ratings equal or more than 1 and number of ratings more than 1000.\n",
    "df.filter((F.col(\"ratings\") >= 1) & (F.col(\"no_of_ratings\") > 1000)) \\\n",
    "  .orderBy(F.asc(\"ratings\"), F.asc(\"no_of_ratings\")) \\\n",
    "  .limit(5) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf589a-7936-454a-8c50-51b5c06e8a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'main_category' and calculate summary statistics for prices\n",
    "df_price_stats = df.groupBy(\"main_category\").agg(\n",
    "    F.format_number(F.min(\"actual_price\"), 2).alias(\"min_actual_price\"),\n",
    "    F.format_number(F.max(\"actual_price\"), 2).alias(\"max_actual_price\"),\n",
    "    F.format_number(F.avg(\"actual_price\"), 2).alias(\"avg_actual_price\"),\n",
    "    F.format_number(F.min(\"discount_price\"), 2).alias(\"min_discount_price\"),\n",
    "    F.format_number(F.max(\"discount_price\"), 2).alias(\"max_discount_price\"),\n",
    "    F.format_number(F.avg(\"discount_price\"), 2).alias(\"avg_discount_price\"),\n",
    ")\n",
    "\n",
    "# Show dataframe\n",
    "df_price_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd8268f-a574-400e-957c-3154fd3a643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the total discount loss based on actual price - discount price\n",
    "df.withColumn(\"discount_loss\", F.col(\"actual_price\") - F.col(\"discount_price\")) \\\n",
    "  .agg(\n",
    "      F.format_number(F.sum(\"actual_price\"), 2).alias(\"total_actual_price\"),\n",
    "      F.format_number(F.sum(\"discount_price\"), 2).alias(\"total_discount_price\"),\n",
    "      F.format_number(F.sum(\"discount_loss\"), 2).alias(\"total_discount_loss\")\n",
    "  ) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f8114-3f07-4bd6-b9f7-2e5eb93cf35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'main_category' and calculate average rating and total number of ratings\n",
    "df.groupBy(\"main_category\") \\\n",
    "  .agg(\n",
    "      F.round(F.avg(\"ratings\"), 1).alias(\"average_rating\"),\n",
    "      F.sum(\"no_of_ratings\").alias(\"total_number_ratings\")\n",
    "  ) \\\n",
    "  .orderBy(\"total_number_ratings\") \\\n",
    "  .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
